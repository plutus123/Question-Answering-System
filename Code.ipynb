{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b54a848-6acc-47d7-9715-ec15aa704d00",
   "metadata": {},
   "source": [
    "# Web Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0327d9ec-e804-4176-a0e5-fec6aa93bce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /workspace/scraping + llm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current working directory:\", current_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c61fe6b9-5445-4d08-ae5a-e80da431e51d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping URL: https://docs.nvidia.com/cuda/\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/contents.html\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cuda-libraries\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cuda-toolkit-major-component-versions\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#new-features\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#general-cuda\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cuda-compiler\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cuda-developer-tools\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#resolved-issues\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#id2\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#known-issues-and-limitations\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#deprecated-or-dropped-features\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#deprecated-or-dropped-architectures\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#deprecated-operating-systems\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#deprecated-toolchains\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cuda-tools\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cublas-library\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cublas-release-12-5-update-1\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cublas-release-12-5\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cublas-release-12-4-update-1\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cublas-release-12-4\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cublas-release-12-3-update-1\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cublas-release-12-3\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cublas-release-12-2-update-2\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cublas-release-12-2\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cublas-release-12-1-update-1\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cublas-release-12-0-update-1\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cublas-release-12-0\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cufft-library\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cufft-release-12-5\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cufft-release-12-4-update-1\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cufft-release-12-4\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cufft-release-12-3-update-1\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cufft-release-12-3\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cufft-release-12-2\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cufft-release-12-1-update-1\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cufft-release-12-1\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cufft-release-12-0-update-1\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cufft-release-12-0\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cusolver-library\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cusolver-release-12-5-update-1\n",
      "Scraping URL: https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cusolver-release-12-5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Start scraping from the parent URL\u001b[39;00m\n\u001b[1;32m     41\u001b[0m parent_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.nvidia.com/cuda/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 42\u001b[0m \u001b[43mscrape_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Save the parsed data into a text file\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscraped_data.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "Cell \u001b[0;32mIn[2], line 33\u001b[0m, in \u001b[0;36mscrape_data\u001b[0;34m(url, depth)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;66;03m# Ensure the sub_link is within the same domain and is a valid URL\u001b[39;00m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m parsed_url\u001b[38;5;241m.\u001b[39mnetloc \u001b[38;5;241m==\u001b[39m urlparse(url)\u001b[38;5;241m.\u001b[39mnetloc \u001b[38;5;129;01mand\u001b[39;00m parsed_url\u001b[38;5;241m.\u001b[39mscheme \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 33\u001b[0m             \u001b[43mscrape_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_link\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to retrieve URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m due to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 33\u001b[0m, in \u001b[0;36mscrape_data\u001b[0;34m(url, depth)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;66;03m# Ensure the sub_link is within the same domain and is a valid URL\u001b[39;00m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m parsed_url\u001b[38;5;241m.\u001b[39mnetloc \u001b[38;5;241m==\u001b[39m urlparse(url)\u001b[38;5;241m.\u001b[39mnetloc \u001b[38;5;129;01mand\u001b[39;00m parsed_url\u001b[38;5;241m.\u001b[39mscheme \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 33\u001b[0m             \u001b[43mscrape_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_link\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to retrieve URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m due to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "    \u001b[0;31m[... skipping similar frames: scrape_data at line 33 (2 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[2], line 33\u001b[0m, in \u001b[0;36mscrape_data\u001b[0;34m(url, depth)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;66;03m# Ensure the sub_link is within the same domain and is a valid URL\u001b[39;00m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m parsed_url\u001b[38;5;241m.\u001b[39mnetloc \u001b[38;5;241m==\u001b[39m urlparse(url)\u001b[38;5;241m.\u001b[39mnetloc \u001b[38;5;129;01mand\u001b[39;00m parsed_url\u001b[38;5;241m.\u001b[39mscheme \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 33\u001b[0m             \u001b[43mscrape_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_link\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to retrieve URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m due to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m, in \u001b[0;36mscrape_data\u001b[0;34m(url, depth)\u001b[0m\n\u001b[1;32m     14\u001b[0m visited_links\u001b[38;5;241m.\u001b[39madd(url)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 17\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     19\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/urllib3/connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    729\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/urllib3/connectionpool.py:404\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 404\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/urllib3/connectionpool.py:1058\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1058\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n\u001b[1;32m   1061\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1062\u001b[0m         (\n\u001b[1;32m   1063\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnverified HTTPS request is being made to host \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1069\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/urllib3/connection.py:363\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;66;03m# Add certificate verification\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[1;32m    365\u001b[0m     tls_in_tls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msocket_options\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout),\n\u001b[1;32m    183\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[1;32m     84\u001b[0m         sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m     \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sock\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "\n",
    "# To avoid scraping the same URL multiple times, we'll use a set.\n",
    "visited_links = set()\n",
    "scraped_data = []\n",
    "\n",
    "def scrape_data(url, depth):\n",
    "    if depth > 5 or url in visited_links:\n",
    "        return\n",
    "    \n",
    "    visited_links.add(url)\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract the data from the current URL\n",
    "        print(f\"Scraping URL: {url}\")\n",
    "        page_content = soup.get_text(separator=\"\\n\")\n",
    "        # scraped_data.append(f\"URL: {url}\\n{page_content[:500]}\\n{'-'*80}\\n\") #(for testing) I am saving only 500 characters from each page since it is taking too much time to save entire content\n",
    "        scraped_data.append(f\"URL: {url}\\n{page_content}\\n{'-'*80}\\n\")\n",
    "        # Find all sub-links on the current page\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            sub_link = urljoin(url, link['href'])\n",
    "            parsed_url = urlparse(sub_link)\n",
    "\n",
    "            # Ensure the sub_link is within the same domain and is a valid URL\n",
    "            if parsed_url.netloc == urlparse(url).netloc and parsed_url.scheme in [\"http\", \"https\"]:\n",
    "                scrape_data(sub_link, depth + 1)\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to retrieve URL: {url} due to {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while scraping {url}: {e}\")\n",
    "\n",
    "# Start scraping from the parent URL\n",
    "parent_url = \"https://docs.nvidia.com/cuda/\"\n",
    "scrape_data(parent_url, 0)\n",
    "\n",
    "# Save the parsed data into a text file\n",
    "with open('scraped_data.txt', 'w', encoding='utf-8') as file:\n",
    "    for data in scraped_data:\n",
    "        file.write(data)\n",
    "\n",
    "print('Scraping completed. Data saved to scraped_data.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1738f3-3398-4b9b-8dbb-25335efe70cc",
   "metadata": {},
   "source": [
    "# Data Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "211ec630-f627-4f9a-828d-41c4447393dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking completed. Data saved to chunked_text.txt\n"
     ]
    }
   ],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 2000000  # Increase max length to 2 million characters\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    \n",
    "    # Split the data by the separator used in the scraped data file\n",
    "    pages = data.split(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "    parsed_data = []\n",
    "    \n",
    "    for page in pages:\n",
    "        if page.strip():\n",
    "            parts = page.split(\"\\n\", 1)\n",
    "            url = parts[0].replace(\"URL: \", \"\")\n",
    "            text = parts[1] if len(parts) > 1 else \"\"\n",
    "            parsed_data.append({'url': url, 'text': text})\n",
    "    \n",
    "    return parsed_data\n",
    "\n",
    "def chunk_text(text, max_length=1000000):\n",
    "    \"\"\"Split text into smaller chunks of specified maximum length.\"\"\"\n",
    "    for i in range(0, len(text), max_length):\n",
    "        yield text[i:i + max_length]\n",
    "\n",
    "def chunk_data(scraped_data):\n",
    "    chunked_data = []\n",
    "    \n",
    "    for page_data in scraped_data:\n",
    "        chunks = []\n",
    "        text = page_data.get('text', '')\n",
    "        \n",
    "        # Split text into smaller chunks if it's too long\n",
    "        text_chunks = chunk_text(text, max_length=1000000)\n",
    "        \n",
    "        for text_chunk in text_chunks:\n",
    "            # Process text using spaCy for sentence segmentation\n",
    "            doc = nlp(text_chunk)\n",
    "            \n",
    "            # Chunk sentences based on similarity\n",
    "            current_chunk = []\n",
    "            for sent in doc.sents:\n",
    "                if current_chunk:\n",
    "                    # Check semantic similarity between current chunk and new sentence\n",
    "                    chunk_embedding = model.encode(\" \".join([str(s) for s in current_chunk]))\n",
    "                    sent_embedding = model.encode(sent.text)\n",
    "                    similarity = np.dot(chunk_embedding, sent_embedding) / (np.linalg.norm(chunk_embedding) * np.linalg.norm(sent_embedding))\n",
    "                    \n",
    "                    if similarity < 0.7:  # Threshold for semantic similarity\n",
    "                        chunks.append(\" \".join([str(s) for s in current_chunk]))\n",
    "                        current_chunk = []\n",
    "                \n",
    "                current_chunk.append(sent)\n",
    "            \n",
    "            if current_chunk:\n",
    "                chunks.append(\" \".join([str(s) for s in current_chunk]))\n",
    "        \n",
    "        page_data['chunks'] = chunks\n",
    "        chunked_data.append(page_data)\n",
    "    \n",
    "    return chunked_data\n",
    "\n",
    "def save_chunked_data(chunked_data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for page_data in chunked_data:\n",
    "            for chunk in page_data['chunks']:\n",
    "                f.write(chunk + '\\n')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_file = 'scraped_data.txt'\n",
    "    chunked_text_file = 'chunked_text.txt'\n",
    "    \n",
    "    # Load scraped data\n",
    "    scraped_data = load_data(input_file)\n",
    "    \n",
    "    # Chunk the data based on semantic similarity or topics\n",
    "    chunked_data = chunk_data(scraped_data)\n",
    "    \n",
    "    # Save chunked data to text file\n",
    "    save_chunked_data(chunked_data, chunked_text_file)\n",
    "    \n",
    "    print(f'Chunking completed. Data saved to {chunked_text_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e63429c-0d7d-4dec-a37d-b4a53c94d897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking completed. Data saved to chunked_text_small.txt\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 1000000  # Reduced max length for faster processing\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def load_data(file_path, max_pages=5):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    \n",
    "    # Split the data by the separator used in the scraped data file\n",
    "    pages = data.split(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "    parsed_data = []\n",
    "    \n",
    "    for page in pages[:max_pages]:  # Limit to max_pages\n",
    "        if page.strip():\n",
    "            parts = page.split(\"\\n\", 1)\n",
    "            url = parts[0].replace(\"URL: \", \"\")\n",
    "            text = parts[1] if len(parts) > 1 else \"\"\n",
    "            parsed_data.append({'url': url, 'text': text})\n",
    "    \n",
    "    return parsed_data\n",
    "\n",
    "def chunk_data(scraped_data, max_sentences=50):\n",
    "    chunked_data = []\n",
    "    \n",
    "    for page_data in scraped_data:\n",
    "        chunks = []\n",
    "        text = page_data.get('text', '')\n",
    "        \n",
    "        # Process text using spaCy for sentence segmentation\n",
    "        doc = nlp(text[:100000])  # Limit text length for faster processing\n",
    "        \n",
    "        # Chunk sentences based on similarity\n",
    "        current_chunk = []\n",
    "        for sent in list(doc.sents)[:max_sentences]:  # Limit number of sentences\n",
    "            if current_chunk:\n",
    "                # Check semantic similarity between current chunk and new sentence\n",
    "                chunk_embedding = model.encode(\" \".join([str(s) for s in current_chunk]))\n",
    "                sent_embedding = model.encode(sent.text)\n",
    "                similarity = np.dot(chunk_embedding, sent_embedding) / (np.linalg.norm(chunk_embedding) * np.linalg.norm(sent_embedding))\n",
    "                \n",
    "                if similarity < 0.7:  # Threshold for semantic similarity\n",
    "                    chunks.append(\" \".join([str(s) for s in current_chunk]))\n",
    "                    current_chunk = []\n",
    "            \n",
    "            current_chunk.append(sent)\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join([str(s) for s in current_chunk]))\n",
    "        \n",
    "        page_data['chunks'] = chunks\n",
    "        chunked_data.append(page_data)\n",
    "    \n",
    "    return chunked_data\n",
    "\n",
    "def save_chunked_data(chunked_data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for page_data in chunked_data:\n",
    "            for chunk in page_data['chunks']:\n",
    "                f.write(chunk + '\\n')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_file = 'scraped_data.txt'\n",
    "    chunked_text_file = 'chunked_text.txt'\n",
    "    \n",
    "    # Load scraped data (limited to 5 pages)\n",
    "    scraped_data = load_data(input_file, max_pages=5)\n",
    "    \n",
    "    # Chunk the data based on semantic similarity or topics (limited to 50 sentences per page)\n",
    "    chunked_data = chunk_data(scraped_data, max_sentences=50)\n",
    "    \n",
    "    # Save chunked data to text file\n",
    "    save_chunked_data(chunked_data, chunked_text_file)\n",
    "    \n",
    "    print(f'Chunking completed. Data saved to {chunked_text_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2deab28-e088-4f38-9320-87211c4b35a1",
   "metadata": {},
   "source": [
    "# Chunks to Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b03f46f0-82c4-468e-a879-837383e9d9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Creating topic model...\n",
      "Processing chunks and saving data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 3/3 [00:00<00:00, 19.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding conversion and topic modeling completed. Data saved to chunked_data_with_embeddings.txt\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model.to(device)\n",
    "\n",
    "def chunk_generator(file_path, batch_size=1000):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        chunks = []\n",
    "        for line in f:\n",
    "            chunks.append(line.strip())\n",
    "            if len(chunks) == batch_size:\n",
    "                yield chunks\n",
    "                chunks = []\n",
    "        if chunks:\n",
    "            yield chunks\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    return [word for word in text.split() if word not in STOPWORDS]\n",
    "\n",
    "def create_topic_model(chunk_gen, num_topics=10):\n",
    "    dictionary = corpora.Dictionary()\n",
    "    corpus = []\n",
    "    for chunks in chunk_gen:\n",
    "        preprocessed_chunks = [preprocess_text(chunk) for chunk in chunks]\n",
    "        dictionary.add_documents(preprocessed_chunks)\n",
    "        corpus.extend([dictionary.doc2bow(text) for text in preprocessed_chunks])\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=42)\n",
    "    return lda_model, dictionary\n",
    "\n",
    "def get_chunk_topic(chunk, lda_model, dictionary):\n",
    "    bow = dictionary.doc2bow(preprocess_text(chunk))\n",
    "    topics = lda_model.get_document_topics(bow)\n",
    "    return max(topics, key=lambda x: x[1])[0] if topics else None\n",
    "\n",
    "@torch.no_grad()\n",
    "def process_and_save_chunks(chunks, lda_model, dictionary, output_file, batch_size=64):\n",
    "    with open(output_file, 'a', encoding='utf-8') as f:\n",
    "        for i in tqdm(range(0, len(chunks), batch_size), desc=\"Processing batches\"):\n",
    "            batch_chunks = chunks[i:i + batch_size]\n",
    "            try:\n",
    "                embeddings = model.encode(batch_chunks, convert_to_tensor=True, device=device)\n",
    "                topics = [get_chunk_topic(chunk, lda_model, dictionary) for chunk in batch_chunks]\n",
    "                \n",
    "                for chunk, embedding, topic in zip(batch_chunks, embeddings, topics):\n",
    "                    embedding_str = ' '.join(map(str, embedding.cpu().numpy().tolist()))\n",
    "                    f.write(f\"{chunk}\\t{embedding_str}\\t{topic}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {i//batch_size}: {e}\")\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    chunked_text_file = 'chunked_text.txt'\n",
    "    output_file = 'chunked_data_with_embeddings.txt'\n",
    "    \n",
    "    try:\n",
    "        print(\"Creating topic model...\")\n",
    "        lda_model, dictionary = create_topic_model(chunk_generator(chunked_text_file))\n",
    "        \n",
    "        print(\"Processing chunks and saving data...\")\n",
    "        for chunks in chunk_generator(chunked_text_file):\n",
    "            process_and_save_chunks(chunks, lda_model, dictionary, output_file, batch_size=64)\n",
    "            gc.collect()\n",
    "        \n",
    "        print(f'Embedding conversion and topic modeling completed. Data saved to {output_file}')\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9e214ef2-fca1-465d-bbc9-5679f7b49ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query (or 'quit' to exit):  What is Cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search Results:\n",
      "1. Chunk: Maxwell is NVIDIAâs 4th-generation architecture for CUDA compute applications....\n",
      "   Topic: 9\n",
      "   URL: https://docs.nvidia.com/cuda/\n",
      "   Score: 0.5\n",
      "---\n",
      "2. Chunk: Pascal is NVIDIAâs 5th-generation architecture for CUDA compute applications....\n",
      "   Topic: 2\n",
      "   URL: https://docs.nvidia.com/cuda/\n",
      "   Score: 0.5\n",
      "---\n",
      "3. Chunk: Volta is NVIDIAâs 6th-generation architecture for CUDA compute applications....\n",
      "   Topic: 5\n",
      "   URL: https://docs.nvidia.com/cuda/\n",
      "   Score: 0.5\n",
      "---\n",
      "4. Chunk: Turing is NVIDIAâs 7th-generation architecture for CUDA compute applications....\n",
      "   Topic: 9\n",
      "   URL: https://docs.nvidia.com/cuda/\n",
      "   Score: 0.5\n",
      "---\n",
      "5. Chunk: This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by le...\n",
      "   Topic: 8\n",
      "   URL: https://docs.nvidia.com/cuda/\n",
      "   Score: 0.45411295605086405\n",
      "---\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query (or 'quit' to exit):  quit\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "# Load the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def load_index_and_metadata(index_type):\n",
    "    index = faiss.read_index(f\"cuda_docs_{index_type.lower()}.index\")\n",
    "    with open(f\"cuda_docs_{index_type.lower()}_metadata.pkl\", 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    return index, metadata\n",
    "\n",
    "def create_bm25_index(metadata):\n",
    "    tokenized_corpus = [doc[0].split() for doc in metadata]\n",
    "    return BM25Okapi(tokenized_corpus)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    return text.lower()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def expand_query(query, top_k=3):\n",
    "    expanded_terms = []\n",
    "    for word in query.split():\n",
    "        synsets = wordnet.synsets(word)\n",
    "        word_expanded = []\n",
    "        for synset in synsets:\n",
    "            for lemma in synset.lemmas():\n",
    "                if lemma.name() != word and lemma.name() not in word_expanded:\n",
    "                    word_expanded.append(lemma.name())\n",
    "                    if len(word_expanded) == top_k:\n",
    "                        break\n",
    "            if len(word_expanded) == top_k:\n",
    "                break\n",
    "        expanded_terms.extend(word_expanded)\n",
    "    return query + ' ' + ' '.join(expanded_terms)\n",
    "\n",
    "def pseudo_relevance_feedback(query_vector, index, metadata, top_k=5, alpha=0.3):\n",
    "    # Perform initial search\n",
    "    distances, indices = index.search(query_vector.reshape(1, -1), top_k)\n",
    "    \n",
    "    # Get the top-k documents\n",
    "    top_docs = [metadata[i][0] for i in indices[0]]\n",
    "    \n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(top_docs)\n",
    "    \n",
    "    # Calculate centroid of top-k documents\n",
    "    centroid = tfidf_matrix.mean(axis=0)\n",
    "    \n",
    "    # Expand query vector\n",
    "    expanded_query_vector = query_vector + alpha * model.encode(vectorizer.get_feature_names_out()[centroid.argmax()])\n",
    "    \n",
    "    return expanded_query_vector\n",
    "\n",
    "def hybrid_search(dense_index, bm25_index, metadata, query, k=5, alpha=0.5, use_query_expansion=True, use_prf=True):\n",
    "    # Preprocess and optionally expand the query\n",
    "    preprocessed_query = preprocess_text(query)\n",
    "    if use_query_expansion:\n",
    "        expanded_query = expand_query(preprocessed_query)\n",
    "    else:\n",
    "        expanded_query = preprocessed_query\n",
    "    \n",
    "    # Dense retrieval\n",
    "    query_vector = model.encode(expanded_query)\n",
    "    if use_prf:\n",
    "        query_vector = pseudo_relevance_feedback(query_vector, dense_index, metadata)\n",
    "    dense_distances, dense_indices = dense_index.search(query_vector.reshape(1, -1), k*2)\n",
    "    \n",
    "    # BM25 retrieval\n",
    "    bm25_scores = bm25_index.get_scores(expanded_query.split())\n",
    "    bm25_top_indices = np.argsort(bm25_scores)[::-1][:k*2]\n",
    "    \n",
    "    # Combine results\n",
    "    combined_scores = {}\n",
    "    for i, idx in enumerate(dense_indices[0]):\n",
    "        combined_scores[idx] = alpha * (1 - dense_distances[0][i])  # Convert distance to similarity\n",
    "    \n",
    "    for i, idx in enumerate(bm25_top_indices):\n",
    "        if idx in combined_scores:\n",
    "            combined_scores[idx] += (1 - alpha) * (bm25_scores[idx] / max(bm25_scores))\n",
    "        else:\n",
    "            combined_scores[idx] = (1 - alpha) * (bm25_scores[idx] / max(bm25_scores))\n",
    "    \n",
    "    # Sort and get top k results\n",
    "    top_indices = sorted(combined_scores, key=combined_scores.get, reverse=True)[:k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        chunk, topic, url = metadata[idx]\n",
    "        results.append({\n",
    "            \"chunk\": chunk,\n",
    "            \"topic\": topic,\n",
    "            \"url\": url,\n",
    "            \"score\": combined_scores[idx]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    index_type = \"FLAT\"  # or \"IVF\"\n",
    "    dense_index, metadata = load_index_and_metadata(index_type)\n",
    "    bm25_index = create_bm25_index(metadata)\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"Enter your query (or 'quit' to exit): \")\n",
    "        if query.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        results = hybrid_search(dense_index, bm25_index, metadata, query, k=5, alpha=0.5, use_query_expansion=True, use_prf=True)\n",
    "        \n",
    "        print(\"\\nSearch Results:\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"{i}. Chunk: {result['chunk'][:100]}...\")\n",
    "            print(f\"   Topic: {result['topic']}\")\n",
    "            print(f\"   URL: {result['url']}\")\n",
    "            print(f\"   Score: {result['score']}\")\n",
    "            print(\"---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7259176c-fcfa-4c84-9289-3901f016b1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question about CUDA (or 'quit' to exit):  What is CUDA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieved Passages:\n",
      "1. Maxwell is NVIDIAâs 4th-generation architecture for CUDA compute applications....\n",
      "2. Pascal is NVIDIAâs 5th-generation architecture for CUDA compute applications....\n",
      "3. Volta is NVIDIAâs 6th-generation architecture for CUDA compute applications....\n",
      "4. Turing is NVIDIAâs 7th-generation architecture for CUDA compute applications....\n",
      "5. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by le...\n",
      "\n",
      "Answer:\n",
      "CUDA stands for Compute Unified Device Architecture. It is a parallel computing platform and application programming interface (API) model created by NVIDIA. CUDA allows developers to utilize the power of NVIDIA GPUs for general-purpose processing tasks, enabling them to accelerate computations that can benefit from parallel processing. \n",
      "\n",
      "In the context provided, CUDA is specifically mentioned in relation to different generations of NVIDIA GPU architectures designed for CUDA compute applications. These architectures include Maxwell, Volta, Turing, and Pascal, each representing a different generation of GPU technology optimized for CUDA programming.\n",
      "---\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question about CUDA (or 'quit' to exit):  quit\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Set your OpenAI API key directly here\n",
    "openai_key = \"OPENAI_KEY\"\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=openai_key)\n",
    "\n",
    "# Initialize SentenceTransformer model\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def prepare_context(results: List[Dict], query: str) -> str:\n",
    "    \"\"\"Prepare the context for the LLM from the search results, ranking by relevance to the query.\"\"\"\n",
    "    # Encode the query and chunks\n",
    "    query_embedding = sentence_model.encode(query)\n",
    "    chunk_embeddings = sentence_model.encode([result['chunk'] for result in results])\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    similarities = np.dot(chunk_embeddings, query_embedding) / (np.linalg.norm(chunk_embeddings, axis=1) * np.linalg.norm(query_embedding))\n",
    "    \n",
    "    # Sort results by similarity\n",
    "    sorted_results = [result for _, result in sorted(zip(similarities, results), key=lambda x: x[0], reverse=True)]\n",
    "    \n",
    "    context = \"Here are some relevant passages from the CUDA documentation, ordered by relevance:\\n\\n\"\n",
    "    for i, result in enumerate(sorted_results, 1):\n",
    "        context += f\"{i}. {result['chunk']}\\n\\n\"\n",
    "    return context\n",
    "\n",
    "def answer_question(query: str, results: List[Dict]) -> str:\n",
    "    \"\"\"Use GPT to answer the question based on the retrieved and ranked results.\"\"\"\n",
    "    context = prepare_context(results, query)\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are a helpful assistant that answers questions about CUDA based on the provided context. \n",
    "        Follow these guidelines:\n",
    "        1. Always base your answers on the information provided in the context.\n",
    "        2. If the answer cannot be found in the context, clearly state that you don't have enough information to answer accurately.\n",
    "        3. If the context contains conflicting information, mention this and explain the different viewpoints.\n",
    "        4. Use technical terms correctly and explain them if they're complex.\n",
    "        5. If appropriate, structure your answer with bullet points or numbered lists for clarity.\n",
    "        6. Cite the relevant passage numbers from the context to support your answer.\n",
    "        7. If the user's question is unclear, ask for clarification before attempting to answer.\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        max_tokens=1000,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.3,  # Lower temperature for more focused answers\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def main():\n",
    "    index_type = \"FLAT\"  # or \"IVF\"\n",
    "    dense_index, metadata = load_index_and_metadata(index_type)  # Assuming these functions are defined elsewhere\n",
    "    bm25_index = create_bm25_index(metadata)  # Assuming this function is defined elsewhere\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"Enter your question about CUDA (or 'quit' to exit): \")\n",
    "        if query.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        results = hybrid_search(dense_index, bm25_index, metadata, query, k=5, alpha=0.5, use_query_expansion=True, use_prf=True)\n",
    "        \n",
    "        print(\"\\nRetrieved Passages:\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"{i}. {result['chunk'][:100]}...\")\n",
    "        \n",
    "        answer = answer_question(query, results)\n",
    "        print(\"\\nAnswer:\")\n",
    "        print(answer)\n",
    "        print(\"---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f1ea7fb4-0747-4163-9864-e5ca8cb2d815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gradio/blocks.py:953: UserWarning: Cannot load huggingface. Caught Exception: 404 Client Error: Not Found for url: https://huggingface.co/api/spaces/huggingface (Request ID: Root=1-66995b1d-37ab34102d5caee3577b4f37;c1b3a6c3-872c-40a5-a683-2bd0370ead79)\n",
      "\n",
      "Sorry, we can't find the page you are looking for.\n",
      "  warnings.warn(f\"Cannot load {theme}. Caught Exception: {str(e)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7874\n",
      "Running on public URL: https://7a6481bb5c5242d47f.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://7a6481bb5c5242d47f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from QALLM import answer\n",
    "from VectorDatabase-Retrieval-Reranking import hybrid_search\n",
    "def main():\n",
    "    index_type = \"FLAT\"  # or \"IVF\"\n",
    "    dense_index, metadata = load_index_and_metadata(index_type)  # Assuming these functions are defined elsewhere\n",
    "    bm25_index = create_bm25_index(metadata)  # Assuming this function is defined elsewhere\n",
    "    \n",
    "    def inference(query):\n",
    "        nonlocal dense_index, bm25_index, metadata\n",
    "        results = hybrid_search(dense_index, bm25_index, metadata, query, k=5, alpha=0.5, use_query_expansion=True, use_prf=True)\n",
    "        answer = answer_question(query, results)\n",
    "        return answer\n",
    "\n",
    "    iface = gr.Interface(\n",
    "        fn=inference,\n",
    "        inputs=\"text\",\n",
    "        outputs=\"text\",\n",
    "        title=\"CUDA Documentation Assistant\",\n",
    "        description=\"Ask a question about CUDA documentation.\",\n",
    "        theme=\"huggingface\",\n",
    "        examples=[[\"How to use CUDA with Python?\"]],\n",
    "    )\n",
    "    iface.launch(share=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8875ba9-2e9b-459d-8893-e4915b28f430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e21d1c-1093-41a7-80c0-e2dde1eccf3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1ca997-5dcf-4ae6-b5f2-b1cfbb3d83d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba1fb7c-6cb9-4540-9970-61c840b27e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0538277c-276c-44e2-9d5e-78dbf5ef049a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e808ec-3206-4187-b6be-d0cae9d27413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
